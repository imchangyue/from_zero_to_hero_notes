{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f08493ce-f08f-4f8f-add6-7dee6c8b965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb9774fb-c675-49e7-91cb-2d201284b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children = (),_op = ''):\n",
    "        self.data = data\n",
    "        self._prev = _children\n",
    "        self._op = _op\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda : None\n",
    "    def __mul__(self,other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def __rmul__(self, other):\n",
    "        return self + other\n",
    "    def __add__(self,other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1m\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "    \n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other,(int, float)),'Only supporting int/float powers for now'\n",
    "        out = Value(self.data **other,(self,) ,f'**{other}')\n",
    "        def _backward():\n",
    "            self.grad += other * self.data ** (other-1) *out.grad #时刻注意链式法则 \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x)-1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t,(self,),'tanh')\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward();\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13748bff-e880-4469-a475-aeb7bac16dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.707106777774492\n",
      "---\n",
      "x2 0.5000000048253751\n",
      "w2 0.0\n",
      "x1 -1.5000000144761252\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "#python中默认创建float64，所以要使得一致，在torch中设置dtype = double\n",
    "# Create tensors with requires_grad set correctly\n",
    "x1 = torch.tensor([2.0], dtype=torch.double, requires_grad=True)\n",
    "x2 = torch.tensor([0.0], dtype=torch.double, requires_grad=True)\n",
    "w1 = torch.tensor([-3.0], dtype=torch.double, requires_grad=True)\n",
    "w2 = torch.tensor([1.0], dtype=torch.double, requires_grad=True)\n",
    "b = torch.tensor([6.881373580195432], dtype=torch.double, requires_grad=True)\n",
    "\n",
    "# Perform operations\n",
    "n = x1 * w1 + x2 * w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "# Perform backward pass\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "# Print gradients\n",
    "print('---')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21d30468-037b-4ca4-bec0-a21ec2f05867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "    def __call__(self, x):\n",
    "        act = sum((wi * xi for wi,xi in zip(self.w,x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):  #nin（输入数量）和 nout（该层神经元的数量）\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]  #n 代表 Layer 类中的一个 Neuron 对象\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for neuron in self.neurons:\n",
    "            params.extend(neuron.parameters())\n",
    "        return params\n",
    "    \n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] +nouts  #nin:输入层的单元数，nouts：一个列表，放置除了输入层以外的其他层的单元数\n",
    "        self.layers = [Layer(sz[i],sz[i+1]) for i in range(len(nouts))]\n",
    "    def __call__(self, x):  #x:输入层的单元，用list表示\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17b93169-6132-43e9-afc4-410fe9c14d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.09141464019448592),\n",
       " Value(data=0.7777402278960779),\n",
       " Value(data=0.4752302694998408),\n",
       " Value(data=0.2473189789887785)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = MLP(3,[4,4,1])\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [6.0, -5.0, -1.0, 1.0]\n",
    "ypred = [n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5402d24f-a140-42e9-89ce-d7ac9afd8e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].neurons[0].w[0].grad    #特定神经元的特定权重的梯度是正的，可知：对loss的影响也是正的：\n",
    "#若增加这个权重，那么loss将会增大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0840ebad-a05e-42aa-b04f-ef980cbcdc0a",
   "metadata": {},
   "source": [
    "据此对每个参数（即w）进行微调，以减小loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd6b4eeb-6fc7-4a1c-89c0-b953f3222306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41.00309014984604\n",
      "1 41.00307899752153\n",
      "2 41.00306792760993\n",
      "3 41.003056939192035\n",
      "4 41.003046031362295\n",
      "5 41.00303520322866\n",
      "6 41.00302445391219\n",
      "7 41.00301378254689\n",
      "8 41.003003188279486\n",
      "9 41.00299267026917\n",
      "10 41.00298222768734\n",
      "11 41.00297185971746\n",
      "12 41.00296156555476\n",
      "13 41.002951344406036\n",
      "14 41.00294119548947\n",
      "15 41.00293111803448\n",
      "16 41.00292111128137\n",
      "17 41.002911174481255\n",
      "18 41.00290130689587\n",
      "19 41.002891507797294\n",
      "20 41.002881776467866\n",
      "21 41.00287211219995\n",
      "22 41.002862514295764\n",
      "23 41.002852982067246\n",
      "24 41.00284351483582\n",
      "25 41.00283411193227\n",
      "26 41.002824772696634\n",
      "27 41.002815496477915\n",
      "28 41.002806282634054\n",
      "29 41.0027971305317\n",
      "30 41.0027880395461\n",
      "31 41.002779009060916\n",
      "32 41.00277003846814\n",
      "33 41.002761127167915\n",
      "34 41.00275227456838\n",
      "35 41.00274348008559\n",
      "36 41.00273474314336\n",
      "37 41.00272606317309\n",
      "38 41.002717439613726\n",
      "39 41.00270887191155\n",
      "40 41.00270035952017\n",
      "41 41.00269190190024\n",
      "42 41.00268349851952\n",
      "43 41.0026751488526\n",
      "44 41.00266685238093\n",
      "45 41.00265860859262\n",
      "46 41.00265041698233\n",
      "47 41.00264227705125\n",
      "48 41.00263418830688\n",
      "49 41.00262615026303\n"
     ]
    }
   ],
   "source": [
    "for k in range(50):\n",
    "    #forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum((yout - ygt)**2 for ygt,yout in zip(ys, ypred))\n",
    "    \n",
    "    #backward pass\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "    \n",
    "    #update \n",
    "    for p in n.parameters():\n",
    "        p.data += -0.08 * p.grad  #根据上面的例子对参数进行微调\n",
    "    print(k,loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59afa46f-3dca-4707-8cfe-25482944336c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n.parameters())  #n这个神经网络共有41个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ba5a2d4-2217-4cab-ab4a-c6cedeb5ed34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9999515266754302),\n",
       " Value(data=-0.9997323439824924),\n",
       " Value(data=-0.9996973439917927),\n",
       " Value(data=0.9999426659318282)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d13052-0981-42ea-8276-533e6fe2b2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
